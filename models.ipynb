{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d9b3aa",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4446b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import pyro.distributions as dist\n",
    "from typing import Any, Dict\n",
    "import pyro\n",
    "\n",
    "class LikelihoodDist(enum.Enum):\n",
    "  NORMAL = 'NORMAL'\n",
    "  NB = 'NB'\n",
    "  ZINB = 'ZINB'\n",
    "\n",
    "def make_seasonal_frequencies(\n",
    "    seasonality_periods: np.ndarray, num_harmonics: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "  \"\"\"Return unique Fourier frequencies for given periods and harmonics.\"\"\"\n",
    "  seasonality_periods = np.array(seasonality_periods, dtype=np.float32)\n",
    "  num_harmonics = np.array(num_harmonics, dtype=np.float32)  \n",
    "  if np.any((num_harmonics > seasonality_periods / 2)):\n",
    "    raise ValueError('Harmonic cannot exceed half seasonal period.')\n",
    "  if seasonality_periods.shape != num_harmonics.shape:\n",
    "    raise ValueError('Number of seasonal periods and harmonics must be equal.')\n",
    "  if len(num_harmonics.shape) != 1:\n",
    "    raise ValueError(\n",
    "        'Arguments `num_harmonics` and `seasonality_periods` must be rank 1.'\n",
    "    )\n",
    "  if seasonality_periods.shape[0] == 0:\n",
    "    return (np.zeros(0), np.zeros(0))\n",
    "  harmonics = [np.arange(1, h + 1, dtype=np.float32) for h in num_harmonics]\n",
    "  frequencies = np.concatenate(\n",
    "      [h / p for (h, p) in zip(harmonics, seasonality_periods)]\n",
    "  )\n",
    "  _, idx = np.unique(frequencies, return_index=True)\n",
    "  idx_sort = np.sort(idx)\n",
    "  unique_frequencies = frequencies[idx_sort]\n",
    "  unique_harmonics = np.concatenate(harmonics)[idx_sort]\n",
    "  return (unique_frequencies, unique_harmonics)\n",
    "\n",
    "\n",
    "def make_seasonal_features(\n",
    "    x: torch.Tensor, \n",
    "    seasonality_periods: torch.Tensor, \n",
    "    num_harmonics: torch.Tensor, \n",
    "    rescale: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Возвращает набор косинусных и синусных признаков для каждого сезона.\"\"\"\n",
    "    \n",
    "    x = x.reshape(-1, 1)\n",
    "    \n",
    "    frequencies, harmonics = make_seasonal_frequencies(seasonality_periods, num_harmonics)\n",
    "    \n",
    "    y = 2 * np.pi * frequencies * x.numpy() \n",
    "    features = np.column_stack((np.cos(y), np.sin(y)))\n",
    "    \n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    \n",
    "    denominator = torch.tensor(np.tile(harmonics, 2), dtype=torch.float32)  \n",
    "    return features / denominator if rescale else features\n",
    "\n",
    "\n",
    "def make_fourier_features(x, max_degree, rescale=False):\n",
    "    \"\"\"Создает набор синусоидальных и косинусных признаков.\"\"\"\n",
    "    x = x.view(-1, 1)  \n",
    "    degrees = torch.arange(max_degree, dtype=torch.float32)\n",
    "    y = 2 * torch.pi * 2**degrees * x\n",
    "    \n",
    "    features = torch.cat((torch.cos(y), torch.sin(y)), dim=-1)\n",
    "    \n",
    "    if rescale:\n",
    "        denominator = torch.cat((degrees + 1, degrees + 1))\n",
    "        features = features / denominator\n",
    "    \n",
    "    return features\n",
    "\n",
    "from torch.distributions import Uniform, TransformedDistribution, SigmoidTransform, AffineTransform\n",
    "\n",
    "def logistic_distribution(loc: float, scale: float):\n",
    "    \"\"\"Создает логистическое распределение в PyTorch.\"\"\"\n",
    "    base_dist = Uniform(0.0, 1.0)  \n",
    "    logistic_dist = TransformedDistribution(\n",
    "        base_dist, [\n",
    "            SigmoidTransform().inv, \n",
    "            AffineTransform(loc=loc, scale=scale)  \n",
    "        ]\n",
    "    )\n",
    "    return logistic_dist\n",
    "def prior_model_fn(mlp_template):\n",
    "    log_noise_scale = pyro.sample(\"log_noise_scale\", dist.Logistic(0.0, 10.0))\n",
    "    shape = pyro.sample(\"shape\", dist.Logistic(-1.5, 10.0))\n",
    "    inflated_loc_probs = pyro.sample(\"inflated_loc_probs\", dist.Logistic(0.0, 10.0))\n",
    "    flat_mlp_params = {}\n",
    "    for name, param in mlp_template.items():\n",
    "        weights = pyro.sample(f\"weights_{name}\", dist.Logistic(0.0, torch.ones_like(param)))\n",
    "        flat_mlp_params[name] = weights\n",
    "    \n",
    "    return log_noise_scale, shape, inflated_loc_probs, flat_mlp_params\n",
    "import torch.nn.functional as F\n",
    "from pyro.distributions import (\n",
    "    Normal, NegativeBinomial, ZeroInflatedNegativeBinomial, Independent\n",
    ")\n",
    "\n",
    "from pyro.distributions import ZeroInflatedNegativeBinomial\n",
    "\n",
    "def make_likelihood_model(params, x, mlp,mlp_template, distribution):\n",
    "    if distribution == LikelihoodDist.NORMAL:\n",
    "        if isinstance(params[0], torch.Tensor):\n",
    "            first_length = params[0].shape[0] if params[0].ndim > 0 else 1\n",
    "        else:\n",
    "            first_length = len(params[0]) if hasattr(params[0], '__len__') else 1\n",
    "        if first_length>1:\n",
    "            log_noise_scale = params[0].mean()\n",
    "        else:\n",
    "            log_noise_scale = params[0]\n",
    "        model_state_dict = mlp.state_dict()\n",
    "        new_params_values = params[3:]\n",
    "        model_state_dict.update(zip(model_state_dict.keys(), new_params_values))\n",
    "        mlp.load_state_dict(model_state_dict)\n",
    "        predictions = mlp(x)\n",
    "        noise_scale = 0.01 + torch.exp(torch.tensor(log_noise_scale,dtype=torch.float32))\n",
    "        return torch.distributions.Independent(\n",
    "            torch.distributions.Normal(predictions, noise_scale), 1\n",
    "        )\n",
    "\n",
    "    elif distribution == LikelihoodDist.NB:\n",
    "        if isinstance(params[0], torch.Tensor):\n",
    "            first_length = params[0].shape[0] if params[0].ndim > 0 else 1\n",
    "        else:\n",
    "            first_length = len(params[0]) if hasattr(params[0], '__len__') else 1\n",
    "        if first_length>1:\n",
    "            log_noise_scale = params[0].mean()\n",
    "        else:\n",
    "            log_noise_scale = params[0]\n",
    "        model_state_dict = mlp.state_dict()\n",
    "        new_params_values = params[3:]\n",
    "        model_state_dict.update(zip(model_state_dict.keys(), new_params_values))\n",
    "        mlp.load_state_dict(model_state_dict)\n",
    "        predictions = mlp(x)\n",
    "        mean = torch.nn.functional.softplus(predictions)\n",
    "        shape = torch.nn.functional.softplus(params[1])\n",
    "\n",
    "        neg_binomial = torch.distributions.NegativeBinomial(\n",
    "            total_count=1 / shape, logits=-torch.log(shape) - torch.log(mean)\n",
    "        )\n",
    "        return torch.distributions.Independent(neg_binomial, 1)\n",
    "\n",
    "    elif distribution == LikelihoodDist.ZINB:\n",
    "        if isinstance(params[0], torch.Tensor):\n",
    "            first_length = params[0].shape[0] if params[0].ndim > 0 else 1\n",
    "        else:\n",
    "            first_length = len(params[0]) if hasattr(params[0], '__len__') else 1\n",
    "        if first_length>1:\n",
    "            log_noise_scale = params[0].mean()\n",
    "        else:\n",
    "            log_noise_scale = params[0]\n",
    "        model_state_dict = mlp.state_dict()\n",
    "        new_params_values = params[3:]\n",
    "        model_state_dict.update(zip(model_state_dict.keys(), new_params_values))\n",
    "        mlp.load_state_dict(model_state_dict)\n",
    "        predictions = mlp(x)\n",
    "        mean = torch.nn.functional.softplus(predictions)\n",
    "        shape = torch.nn.functional.softplus(params[1])\n",
    "        inflated_probs = torch.sigmoid(params[2])\n",
    "\n",
    "        zinb_dist = ZeroInflatedNegativeBinomial(\n",
    "            total_count=1 / shape, logits=-torch.log(shape) - torch.log(mean),\n",
    "            gate=inflated_probs\n",
    "        )\n",
    "        return torch.distributions.Independent(zinb_dist, 1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown likelihood distribution: {distribution}\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "class BayesianNeuralField1D(nn.Module):\n",
    "    def __init__(self, width, depth, input_scales, fourier_degrees, interactions,\n",
    "                 num_seasonal_harmonics=None, seasonality_periods=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.input_scales = torch.tensor(input_scales, dtype=torch.float32)\n",
    "        self.fourier_degrees = torch.tensor(fourier_degrees, dtype=torch.int64)\n",
    "        self.interactions = torch.tensor(interactions, dtype=torch.long)\n",
    "        \n",
    "        self.num_seasonal_harmonics = torch.tensor(\n",
    "            num_seasonal_harmonics if num_seasonal_harmonics is not None else [],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        self.seasonality_periods = torch.tensor(\n",
    "            seasonality_periods if seasonality_periods is not None else [],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        self.layer_scales = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(())) for _ in range(depth + 1)\n",
    "        ])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        self.feature_scales = nn.ParameterList()\n",
    "\n",
    "    def make_layer_scale(self, layer_id):\n",
    "        \"\"\"Get the softplus-transformed scale parameter for a specific layer.\"\"\"\n",
    "        return F.softplus(self.layer_scales[layer_id])\n",
    "        \n",
    "    def activation_fn(self, x):\n",
    "        activation_weight = torch.sigmoid(self.logit_activation_weight)\n",
    "        return activation_weight * F.elu(x) + (1 - activation_weight) * torch.tanh(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "        x=torch.tensor(x,dtype=torch.float32)\n",
    "        self.log_scale_adjustment = nn.Parameter(torch.randn(x.shape[-1:], dtype=torch.float32))\n",
    "        \n",
    "        scaled_x = x / (self.input_scales * torch.exp(self.log_scale_adjustment))\n",
    "        \n",
    "        seasonal_features = make_seasonal_features(\n",
    "            x[..., 0], self.seasonality_periods, self.num_seasonal_harmonics, rescale=False\n",
    "        )\n",
    "\n",
    "        fourier_features = [\n",
    "            make_fourier_features(scaled_x[..., i], degree, rescale=False)\n",
    "            for i, degree in enumerate(self.fourier_degrees) if degree > 0\n",
    "        ]\n",
    "\n",
    "        interaction_features = torch.prod(scaled_x[..., self.interactions], dim=-1, keepdim=True)\n",
    "        features = [scaled_x, *fourier_features, seasonal_features, interaction_features]\n",
    "\n",
    "        if len(self.feature_scales) == 0:\n",
    "            for _ in range(len(features)):\n",
    "                self.feature_scales.append(nn.Parameter(torch.randn(())))\n",
    "\n",
    "        features = [\n",
    "            f * F.softplus(self.feature_scales[i]) for i, f in enumerate(features) if f.numel() > 0\n",
    "        ]\n",
    "        self.logit_activation_weight = nn.Parameter(torch.randn(1))\n",
    "\n",
    "        h = torch.cat(features, dim=-1)\n",
    "        if len(self.hidden_layers) == 0:\n",
    "            input_dim = h.shape[-1]\n",
    "            self.hidden_layers = nn.ModuleList([\n",
    "                nn.Linear(self.width if i > 0 else input_dim, self.width) \n",
    "                for i in range(self.depth)\n",
    "            ])\n",
    "            self.output_layer = nn.Linear(self.width, 1)\n",
    "\n",
    "        \n",
    "        for layer_id, layer in enumerate(self.hidden_layers):\n",
    "            layer_scale = self.make_layer_scale(layer_id)\n",
    "            h = h / torch.sqrt(torch.tensor(h.shape[-1], dtype=torch.float32))\n",
    "            h = self.activation_fn(layer_scale * layer(h))\n",
    "\n",
    "        self.output_layer = nn.Linear(self.width, 1)\n",
    "        output_scale = self.make_layer_scale(self.depth)\n",
    "        h = h / torch.sqrt(torch.tensor(h.shape[-1], dtype=torch.float32))\n",
    "        return output_scale * self.output_layer(h).squeeze(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Updated)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

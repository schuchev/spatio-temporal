{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cd4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156a08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Any, Dict, Tuple, List\n",
    "from torch.distributions import Normal\n",
    "tfd = tfp.distributions\n",
    "from scipy.optimize import root_scalar\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "ArrayT = Union[torch.Tensor, np.ndarray]\n",
    "\n",
    "def permute_dataset(features: torch.Tensor, target: torch.Tensor, seed: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    torch.manual_seed(seed)\n",
    "    permutation = torch.randperm(target.size(0))\n",
    "    return features[permutation], target[permutation]\n",
    "\n",
    "def chandrupatla_torch(f, low, high, tol=1e-5, max_iter=60):\n",
    "    a, b = torch.tensor(low).unsqueeze(0), torch.tensor(high).unsqueeze(0)\n",
    "    fa, fb = f(a), f(b)\n",
    "\n",
    "    if (fa * fb > 0).any():\n",
    "        raise ValueError(\"Функция должна иметь разные знаки на концах отрезка.\")\n",
    "\n",
    "    c = a.clone()\n",
    "    fc = fa.clone()\n",
    "    converged = torch.zeros_like(fc, dtype=torch.bool)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        d = (a + b) / 2\n",
    "        fd = f(d)\n",
    "\n",
    "        left_update = (fd * fb < 0)\n",
    "        a = torch.where(left_update, d, a)\n",
    "        fa = torch.where(left_update, fd, fa)\n",
    "\n",
    "        b = torch.where(~left_update, d, b)\n",
    "        fb = torch.where(~left_update, fd, fb)\n",
    "\n",
    "        c = (a + b) / 2\n",
    "        fc = f(c)\n",
    "\n",
    "        new_converged = (torch.abs(fc) < tol) | (torch.abs(b - a) < tol)\n",
    "        converged = converged | new_converged\n",
    "\n",
    "        if converged.all():\n",
    "            return c.squeeze(-1)\n",
    "\n",
    "    raise RuntimeError(\"Не удалось найти корень.\")\n",
    "\n",
    "\n",
    "def _normal_quantile_via_root(means, scales, q, axis=(0, 1)):\n",
    "    n = Normal(means, scales)\n",
    "    \n",
    "    def quantile_root_fn(x):\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "        cdf_val = n.cdf(x_tensor)\n",
    "        \n",
    "        # Проверяем, существует ли указанная ось\n",
    "        if cdf_val.dim() >= max(axis) + 1:\n",
    "            cdf_val = cdf_val.mean(dim=axis)\n",
    "        else:\n",
    "            cdf_val = cdf_val.mean()\n",
    "\n",
    "        return cdf_val - q\n",
    "\n",
    "    low = torch.min(means) - 5 * torch.max(scales)\n",
    "    high = torch.max(means) + 5 * torch.max(scales)\n",
    "\n",
    "    root = chandrupatla_torch(quantile_root_fn, low, high, tol=1e-5, max_iter=60)\n",
    "    return root\n",
    "def _approximate_normal_quantile(means, scales, q, axis=(0, 1)):\n",
    "    if len(means.shape) > 1:\n",
    "        mixture_mean = means.mean(dim=axis)\n",
    "        variance_term = (scales**2 + means**2).mean(dim=axis) - mixture_mean**2\n",
    "    else:\n",
    "        mixture_mean = means.mean()\n",
    "        variance_term = (scales**2 + means**2).mean() - mixture_mean**2\n",
    "\n",
    "    valid_mask = variance_term >= 0\n",
    "    safe_scale = torch.where(valid_mask, torch.sqrt(variance_term), torch.tensor(1.0))\n",
    "\n",
    "    n = Normal(mixture_mean, safe_scale)\n",
    "\n",
    "    quantiles = torch.where(valid_mask, n.icdf(torch.tensor(q)), torch.tensor(float('nan')))\n",
    "    return quantiles\n",
    "\n",
    "def _get_percentile_normal(means, scales, quantiles, axis=None, approximate=False):\n",
    "    if means.dim() > 1:\n",
    "        axis = (0, 1)\n",
    "    else:\n",
    "        axis = (0,)  \n",
    "\n",
    "    quantile_fn = _approximate_normal_quantile if approximate else _normal_quantile_via_root\n",
    "    return [quantile_fn(means, scales, q, axis) for q in quantiles]\n",
    "\n",
    "def _make_forecast_inner(model_args, distribution):\n",
    "    \"\"\"Construct inner forecast function for MAP and VI.\"\"\"\n",
    "    def forecast_inner(params, x_subset):\n",
    "        mlp,mlp_template = make_model(**model_args)\n",
    "        likelihood = models.make_likelihood_model(\n",
    "            params, x_subset, mlp,mlp_template, distribution\n",
    "        )\n",
    "        if distribution == models.LikelihoodDist.NORMAL:\n",
    "            return (likelihood.base_dist.loc, likelihood.base_dist.scale)\n",
    "        elif distribution == models.LikelihoodDist.NB:\n",
    "            return (\n",
    "                likelihood.base_dist.total_count,\n",
    "                likelihood.base_dist.logits,\n",
    "            )\n",
    "        elif distribution == models.LikelihoodDist.ZINB:\n",
    "            return (\n",
    "                likelihood.base_dist.base_dist.total_count,\n",
    "                likelihood.base_dist.base_dist.logits,\n",
    "                likelihood.base_dist.gate_logits,\n",
    "            )\n",
    "        else:\n",
    "            raise TypeError('Distribution must be one of NORMAL, NB, or ZINB.')\n",
    "\n",
    "    return forecast_inner\n",
    "import torch\n",
    "\n",
    "def forecast_parameters_batched(\n",
    "    features: torch.Tensor,\n",
    "    params: dict,\n",
    "    distribution: str,\n",
    "    forecast_inner: callable,\n",
    "    batchsize: int = 1024,\n",
    "):\n",
    "    \"\"\"Вычисление параметров распределения с помощью PyTorch.\"\"\"\n",
    "\n",
    "    forecast_params_slices = [[], [], []]\n",
    "\n",
    "    data_size = features.shape[0]\n",
    "    num_batches = data_size // batchsize\n",
    "    for i in range(num_batches + 1):\n",
    "        if i == num_batches:\n",
    "            batch_slice = slice(i * batchsize, None)\n",
    "            if features[batch_slice].shape[0] == 0:\n",
    "                continue\n",
    "        else:\n",
    "            batch_slice = slice(i * batchsize, (i + 1) * batchsize)\n",
    "\n",
    "        forecast_params = forecast_inner(params, features[batch_slice])\n",
    "        for idx, fc_param in enumerate(forecast_params):\n",
    "            forecast_params_slices[idx].append(fc_param)\n",
    "\n",
    "    if distribution == models.LikelihoodDist.NORMAL:\n",
    "        loc = torch.cat(forecast_params_slices.pop(0), dim=-1)\n",
    "        scale = torch.cat(forecast_params_slices.pop(0), dim=-1)\n",
    "        forecast_params = (loc, scale)\n",
    "\n",
    "    elif distribution == models.LikelihoodDist.NB:\n",
    "        total_count = torch.cat(forecast_params_slices.pop(0), dim=0)\n",
    "        logits = torch.cat(forecast_params_slices.pop(0), dim=0)\n",
    "        forecast_params = (total_count, logits)\n",
    "\n",
    "    elif distribution == models.LikelihoodDist.ZINB:\n",
    "        total_count = forecast_params_slices[0][0]\n",
    "        logits = torch.cat(forecast_params_slices[1], dim=-1)\n",
    "        zero_mass = torch.cat(forecast_params_slices[2], dim=-1)\n",
    "        forecast_params = (total_count, logits, zero_mass)\n",
    "\n",
    "    else:\n",
    "        raise TypeError('Distribution must be NORMAL, NB, or ZINB.')\n",
    "\n",
    "    return forecast_params\n",
    "def softplus_inverse(x):\n",
    "    \"\"\"Реализация обратного Softplus.\"\"\"\n",
    "    return torch.log(torch.exp(x) - 1)\n",
    "\n",
    "\n",
    "def make_vi_init(prior_d):\n",
    "    \"\"\"Construct a surrogate posterior init function.\"\"\"\n",
    "    \n",
    "    trace = pyro.poutine.trace(prior_d).get_trace()\n",
    "    trace.compute_log_prob()\n",
    "    samples = {name: site['value'] for name, site in trace.nodes.items() if site['type'] == 'sample'}\n",
    "    \n",
    "    def _fn(seed=None):\n",
    "        if seed is not None:\n",
    "            pyro.set_rng_seed(seed)\n",
    "\n",
    "        init_values = {}\n",
    "        for i, x in enumerate(samples):\n",
    "            if len(samples[x].shape) != 2:\n",
    "                init_values[f\"zero_initial_mean_for_bias_or_transformed_scale_{i}\"] = pyro.sample(\n",
    "                    f\"zero_initial_mean_for_bias_or_transformed_scale_{i}\",\n",
    "                    dist.Delta(torch.zeros_like(samples[x]))\n",
    "                )\n",
    "            else:\n",
    "                init_values[f\"initial_weight_matrix_{i}\"] = pyro.sample(\n",
    "                    f\"initial_weight_matrix_{i}\",\n",
    "                    dist.TransformedDistribution(\n",
    "                        dist.Uniform(-2, 2),\n",
    "                        dist.transforms.AffineTransform(0.0, torch.ones_like(samples[x]))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            inv_softplus_scale = torch.log(torch.exp(torch.tensor(0.3)) - 1)\n",
    "            init_values[f\"initial_inv_softplus_surrogate_scale_{i}\"] = pyro.sample(\n",
    "                f\"initial_inv_softplus_surrogate_scale_{i}\",\n",
    "                dist.Delta(inv_softplus_scale * torch.ones_like(samples[x]))\n",
    "            )\n",
    "\n",
    "        return init_values\n",
    "    \n",
    "    return _fn\n",
    "\n",
    "def make_model(\n",
    "    width, depth, input_scales, num_seasonal_harmonics, seasonality_periods,\n",
    "    init_x, fourier_degrees, interactions\n",
    "):\n",
    "    mlp = models.BayesianNeuralField1D(\n",
    "        width=width,\n",
    "        depth=depth,\n",
    "        input_scales=input_scales,\n",
    "        fourier_degrees=fourier_degrees,\n",
    "        interactions=interactions,\n",
    "        num_seasonal_harmonics=num_seasonal_harmonics,\n",
    "        seasonality_periods=seasonality_periods,\n",
    "    )\n",
    "\n",
    "    init_input = torch.zeros(init_x, dtype=torch.float32)\n",
    "    mlp(init_input)\n",
    "\n",
    "    return mlp, mlp.state_dict()\n",
    "\n",
    "\n",
    "import pyro.distributions as dist\n",
    "from typing import Any, Dict\n",
    "import pyro\n",
    "import functools\n",
    "def make_prior(**kwargs):\n",
    "    kwargs.pop(\"likelihood_distribution\", None)\n",
    "    mlp_template = make_model(**kwargs)[1]\n",
    "\n",
    "    def prior_model():\n",
    "        return models.prior_model_fn(mlp_template)\n",
    "\n",
    "    return prior_model\n",
    "\n",
    "\n",
    "def build_observation_distribution(distribution, forecast_params):\n",
    "    \"\"\"\n",
    "    Returns (zero inflated) Negative Binomial distribution given parameters.\n",
    "\n",
    "    Args:\n",
    "        distribution: Indicates whether a zero-inflated models.LikelihoodDist.ZINB\n",
    "        or models.LikelihoodDist.NB distribution should be returned.\n",
    "        forecast_params: Tuple of total_count, logits, and optionally maybe_zero_mass\n",
    "    \"\"\"\n",
    "    total_count, logits, *maybe_zero_mass = forecast_params\n",
    "    \n",
    "    if distribution == 'NB': \n",
    "        return NegativeBinomial(total_count=total_count.unsqueeze(-1), logits=logits)\n",
    "    \n",
    "    elif distribution == 'ZINB':  \n",
    "        inflated_loc_probs = maybe_zero_mass[0]\n",
    "        return ZeroInflatedNegativeBinomial(total_count=total_count.unsqueeze(-1), logits=logits, gate=inflated_loc_probs)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown distribution: {distribution}')\n",
    "class ZeroInflatedNegativeBinomial:\n",
    "    def __init__(self, total_count, probs, gate):\n",
    "        self.base_dist = NegativeBinomial(total_count=total_count.item(), probs=probs.item())\n",
    "        self.gate = gate\n",
    "\n",
    "    def cdf(self, x):\n",
    "        return custom_cdf(self, x)\n",
    "\n",
    "    def prob_zero(self):\n",
    "        \"\"\"Calculate the probability of zero occurrences.\"\"\"\n",
    "        pmf_zero = self.base_dist.log_prob(torch.tensor(0)).exp()\n",
    "        return self.gate + (1 - self.gate) * pmf_zero\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.base_dist.mean\n",
    "\n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return self.base_dist.stddev  \n",
    "def custom_cdf(dist, x):\n",
    "    \"\"\"Compute the CDF for Zero-Inflated Negative Binomial distribution.\"\"\"\n",
    "    pmf_values = dist.base_dist.log_prob(torch.arange(0, x.max().item() + 1)).exp()\n",
    "    cdf_values = pmf_values.cumsum(dim=0)\n",
    "    \n",
    "    return dist.gate + (1 - dist.gate) * cdf_values[x.long()]\n",
    "\n",
    "def negative_binomial_cdf(n, p, x):\n",
    "    \"\"\"Custom CDF for Negative Binomial distribution.\"\"\"\n",
    "    x_tensor = x if isinstance(x, torch.Tensor) else torch.tensor(x)\n",
    "    \n",
    "    pmf_values = [torch.exp(NegativeBinomial(total_count=n.item(), probs=p.item()).log_prob(torch.tensor(k))) for k in range(int(x_tensor.max().item()) + 1)]\n",
    "    \n",
    "    cdf_values = torch.tensor(pmf_values).cumsum(dim=0)\n",
    "    \n",
    "    return cdf_values[x_tensor.long()]\n",
    "\n",
    "def _get_nb_quantiles_root(dist, q, ensemble_axes=(0, 1, 2)):\n",
    "    \"\"\"Returns (zero inflated) Negative Binomial quantiles via root-finding.\"\"\"\n",
    "    \n",
    "    def cdf_diff(x):\n",
    "        if isinstance(dist, ZeroInflatedNegativeBinomial):\n",
    "            cdf_val = custom_cdf(dist, x)\n",
    "        else:  \n",
    "            cdf_val = negative_binomial_cdf(dist.total_count.unsqueeze(0), dist.probs.unsqueeze(0), x)  \n",
    "            \n",
    "        if cdf_val.dim() > 1:\n",
    "            return cdf_val.mean(dim=ensemble_axes) - q\n",
    "        else:\n",
    "            return cdf_val - q\n",
    "\n",
    "    low = 0.0\n",
    "    high_mean = dist.mean if isinstance(dist.mean, torch.Tensor) else torch.tensor(dist.mean)\n",
    "    high_stddev = dist.stddev if isinstance(dist.stddev, torch.Tensor) else torch.tensor(dist.stddev)\n",
    "    \n",
    "    if isinstance(dist, ZeroInflatedNegativeBinomial):\n",
    "        high = (high_mean + 1.1 * torch.sqrt(1 - q) * high_stddev).max()\n",
    "    else:  \n",
    "        high = (high_mean + 1.1 * high_stddev).max()\n",
    "\n",
    "    res = chandrupatla_torch(cdf_diff, low, high)\n",
    "\n",
    "    if isinstance(dist, ZeroInflatedNegativeBinomial):\n",
    "        return torch.ceil(torch.where(dist.prob_zero() > q, 0, res))\n",
    "    else: \n",
    "        return torch.ceil(res)\n",
    "def fit_vi(\n",
    "    features: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    seed: int,\n",
    "    observation_model: str,\n",
    "    model_args: dict[str, Any],\n",
    "    ensemble_size: int,\n",
    "    learning_rate: float,\n",
    "    num_epochs: int,\n",
    "    sample_size_divergence: int,\n",
    "    sample_size_posterior: int,\n",
    "    kl_weight: float,\n",
    "    batch_size: Union[int,None] = None,\n",
    "):\n",
    "  \"\"\"Fit BNF using an ensemble VI.\"\"\"\n",
    "  distribution = models.LikelihoodDist(observation_model)\n",
    "\n",
    "  def _neg_energy_fn(params, x, y):\n",
    "    return models.make_likelihood_model(\n",
    "        params, x, *make_model(**model_args), distribution\n",
    "    ).log_prob(y).sum()\n",
    "\n",
    "  return ensemble_vi(\n",
    "      features,\n",
    "      target,\n",
    "      _neg_energy_fn,\n",
    "      prior_d=make_prior(**model_args),\n",
    "      ensemble_size=ensemble_size,\n",
    "      learning_rate=learning_rate,\n",
    "      num_epochs=num_epochs,\n",
    "      seed=seed,\n",
    "      sample_size=sample_size_divergence,\n",
    "      num_samples=sample_size_posterior,\n",
    "      kl_weight=kl_weight,\n",
    "      batch_size=batch_size,\n",
    "  )\n",
    "\n",
    "def fit_map(\n",
    "    features: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    seed: int,\n",
    "    observation_model: str,\n",
    "    model_args: dict[str, any],\n",
    "    num_particles: int,\n",
    "    learning_rate: float,\n",
    "    num_epochs: int,\n",
    "    prior_weight: float = 1.0,\n",
    "    batch_size: Union[int, None] = None,\n",
    "    num_splits: int = 1,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    distribution = models.LikelihoodDist(observation_model)\n",
    "    \n",
    "    def _neg_energy_fn(params, x, y):\n",
    "        model = models.make_likelihood_model(\n",
    "            params, x, *make_model(**model_args), distribution\n",
    "        )\n",
    "        log_prob = model.log_prob(y).sum()\n",
    "        return log_prob\n",
    "    \n",
    "    target_scale = torch.std(target[~torch.isnan(target)])\n",
    "    \n",
    "    def _make_init_fn(prior_d):\n",
    "        \"\"\"Construct a surrogate posterior init function.\"\"\"\n",
    "        trace = pyro.poutine.trace(prior_d).get_trace()\n",
    "        trace.compute_log_prob()\n",
    "        samples = {name: site['value'] for name, site in trace.nodes.items() if site['type'] == 'sample'}\n",
    "\n",
    "        def _fn(seed=None):\n",
    "            if seed is not None:\n",
    "                pyro.set_rng_seed(seed)\n",
    "\n",
    "            init_values = {}\n",
    "            for i, x in enumerate(samples):\n",
    "                if i == 0:\n",
    "                    init_values[f\"zero_initial_log_noise_scale\"] = pyro.sample(\n",
    "                        f\"zero_initial_log_noise_scale\",\n",
    "                        dist.Delta(torch.ones_like(samples[x]) * torch.log(torch.tensor(0.5) / 2))\n",
    "                    )\n",
    "                elif len(samples[x].shape) != 2:\n",
    "                    init_values[f\"zero_initial_mean_for_bias_or_transformed_scale_{i}\"] = pyro.sample(\n",
    "                        f\"zero_initial_mean_for_bias_or_transformed_scale_{i}\",\n",
    "                        dist.Delta(torch.zeros_like(samples[x]))\n",
    "                    )\n",
    "                else:\n",
    "                    init_values[f\"initial_weight_matrix_{i}\"] = pyro.sample(\n",
    "                        f\"initial_weight_matrix_{i}\",\n",
    "                        dist.TransformedDistribution(\n",
    "                            dist.Uniform(-2, 2),\n",
    "                            dist.transforms.AffineTransform(0.0, torch.ones_like(samples[x]))\n",
    "                        )\n",
    "                    )\n",
    "            return init_values\n",
    "\n",
    "        return _fn\n",
    "    \n",
    "    prior = make_prior(**model_args)\n",
    "    params = []\n",
    "    losses = []\n",
    "    for i in range(num_splits):\n",
    "        seed_i = seed + i if num_splits > 1 else seed\n",
    "\n",
    "        init_fn = _make_init_fn(prior)\n",
    "        ensemble_size = num_particles // num_splits\n",
    "\n",
    "        params_i, losses_i = ensemble_map(\n",
    "            features,\n",
    "            target,\n",
    "            _neg_energy_fn,\n",
    "            prior_d=prior,\n",
    "            init_fn=init_fn,\n",
    "            ensemble_size=ensemble_size,\n",
    "            learning_rate=learning_rate,\n",
    "            num_epochs=num_epochs,\n",
    "            seed=seed_i,\n",
    "            batch_size=batch_size,\n",
    "            prior_weight=prior_weight,\n",
    "        )\n",
    "\n",
    "        params.extend([{\n",
    "            k: v.detach() for k, v in p.items()\n",
    "        } for p in params_i])\n",
    "\n",
    "        losses.extend(losses_i)\n",
    "\n",
    "    params_dict = {}\n",
    "    for p in params:\n",
    "        for k, v in p.items():\n",
    "            if k not in params_dict:\n",
    "                params_dict[k] = []\n",
    "            params_dict[k].append(v) \n",
    "\n",
    "    params = {k: torch.stack(v, dim=0) for k, v in params_dict.items()}\n",
    "    \n",
    "    losses = torch.cat(losses, dim=0)  \n",
    "\n",
    "    return params, losses\n",
    "\n",
    "\n",
    "def predict_bnf(\n",
    "    features: torch.Tensor,\n",
    "    observation_model: str,\n",
    "    params: dict[str, torch.Tensor],  \n",
    "    model_args: dict[str, any],\n",
    "    quantiles: torch.Tensor,\n",
    "    ensemble_dims: int = 2,\n",
    "    approximate_quantiles: bool = False,\n",
    ") -> tuple[torch.Tensor, list[torch.Tensor]]:\n",
    "    \"\"\"Predict new data from an existing BNF fit.\"\"\"\n",
    "    distribution = models.LikelihoodDist(observation_model)\n",
    "    assert ensemble_dims >= 1\n",
    "\n",
    "    forecast_inner = _make_forecast_inner(model_args, distribution)\n",
    "\n",
    "    forecast_params_list = []\n",
    "    for i in range(ensemble_dims):\n",
    "        params_i = {k: v[i] for k, v in params.items()}  \n",
    "        forecast_params_i = forecast_inner(list(params_i.values()), features)  \n",
    "\n",
    "        if isinstance(forecast_params_i, tuple):\n",
    "            forecast_params_list.extend(forecast_params_i) \n",
    "        else:\n",
    "            forecast_params_list.append(forecast_params_i) \n",
    "\n",
    "    forecast_params = torch.stack(forecast_params_list, dim=0)\n",
    "\n",
    "    if distribution == models.LikelihoodDist.NORMAL:\n",
    "        means, scales = forecast_params[0], forecast_params[1]\n",
    "        forecast_means = means\n",
    "        forecast_quantiles = _get_percentile_normal(\n",
    "            forecast_means,\n",
    "            scales,\n",
    "            quantiles,\n",
    "            axis=(0, 1),  \n",
    "            approximate=approximate_quantiles,\n",
    "        )\n",
    "\n",
    "    elif distribution in [models.LikelihoodDist.NB, models.LikelihoodDist.ZINB]:\n",
    "        obs_d = _build_observation_distribution(distribution, forecast_params)\n",
    "        forecast_means = obs_d.mean()\n",
    "        forecast_quantiles = torch.stack(\n",
    "            [ _get_nb_quantiles_root(obs_d, q, ensemble_axes=(0, 1)) for q in quantiles]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unknown distribution: {distribution}')\n",
    "\n",
    "    return forecast_means, forecast_quantiles\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def compute_prior_log_prob(prior_samples):\n",
    "    i=0\n",
    "    log_prob_values=[]\n",
    "    for name, param in prior_samples.items():\n",
    "        if i==0:\n",
    "            log_prob_values.append(dist.Logistic(0.0, 1.0).log_prob(param).sum())\n",
    "        elif i==1:\n",
    "            log_prob_values.append(dist.Logistic(-1.5, 1.0).log_prob(param).sum())\n",
    "        elif i==2:\n",
    "            log_prob_values.append(dist.Logistic(0.0, 1.0).log_prob(param).sum())\n",
    "        else:\n",
    "            log_prob_values.append(dist.Logistic(-1, torch.ones_like(param)).log_prob(param).sum())\n",
    "        i+=1\n",
    "\n",
    "    return sum(log_prob_values)\n",
    "def ensemble_map(\n",
    "    features: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    neg_energy_fn: callable,\n",
    "    prior_d: torch.distributions.Distribution,\n",
    "    init_fn: callable,\n",
    "    ensemble_size: int,\n",
    "    learning_rate: float,\n",
    "    num_epochs: int,\n",
    "    seed: int,\n",
    "    batch_size: Union[int,None] = None,\n",
    "    prior_weight: float = 1.0,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Fit an ensemble of MAP estimates in PyTorch.\"\"\"\n",
    "    \n",
    "    if batch_size is None:\n",
    "        batch_size = target.shape[0]\n",
    "    \n",
    "    device = features.device\n",
    "    \n",
    "    \n",
    "    init_seeds = torch.randint(0, 2**32 - 1, (1, ensemble_size), dtype=torch.int64)\n",
    "    init_params = [init_fn(seed.item()) for seed in init_seeds[0]]\n",
    "    dataset = TensorDataset(features, target)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    all_losses = []\n",
    "    def _target_log_prob_fn(params, x_batch, y_batch, neg_energy_fn, prior_d, prior_weight, batch_size):\n",
    "        neg_log_likelihood = neg_energy_fn(list(params.values()), x_batch, y_batch)\n",
    "\n",
    "        if prior_weight == 0.0:\n",
    "            return -neg_log_likelihood * (y_batch.shape[0] / batch_size)\n",
    "        \n",
    "    \n",
    "        prior_log_prob = compute_prior_log_prob(params)\n",
    "        return -(neg_log_likelihood * (y_batch.shape[0] / batch_size) + prior_log_prob * prior_weight)\n",
    "    for param_set in init_params:\n",
    "        for key, value in param_set.items():\n",
    "            if torch.all(value == 0):  \n",
    "                param_set[key] = torch.nn.Parameter(value + torch.randn_like(value) * 1e-3, requires_grad=True)\n",
    "            else:\n",
    "                param_set[key] = torch.nn.Parameter(value.clone(), requires_grad=True)\n",
    "    param_groups = [{'params': list(params.values())} for params in init_params]\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(param_groups, lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        for batch_features, batch_target in data_loader:\n",
    "            batch_features, batch_target = batch_features.to(device), batch_target.to(device)\n",
    "\n",
    "            losses = []\n",
    "            for params in init_params:\n",
    "                optimizer.zero_grad()\n",
    "                loss = _target_log_prob_fn(params, batch_features, batch_target, neg_energy_fn, prior_d, prior_weight, batch_size)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                losses.append(loss.item())\n",
    "            epoch_losses.append(losses)\n",
    "        all_losses.append(epoch_losses)\n",
    "    updated_params = [{k: v.detach().clone() for k, v in params.items()} for params in init_params]\n",
    "    return updated_params, torch.tensor(all_losses)\n",
    "def ensemble_vi(\n",
    "    features: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    neg_energy_fn: Callable[[torch.Tensor, torch.Tensor, torch.Tensor], float],\n",
    "    prior_d: Callable[[torch.Tensor], Normal],\n",
    "    ensemble_size: int,\n",
    "    learning_rate: float,\n",
    "    num_epochs: int,\n",
    "    seed: int,\n",
    "    sample_size: int = 10,\n",
    "    num_samples: int = 30,\n",
    "    kl_weight: float = 1.0,\n",
    "    batch_size: int = None,\n",
    ") -> Tuple[torch.nn.Module, torch.Tensor, torch.Tensor]:\n",
    "    features=torch.tensor(features)\n",
    "    target=torch.tensor(target)\n",
    "    def _target_log_prob_fn_inner(params, x_batch, y_batch):\n",
    "        return compute_prior_log_prob(params) + (\n",
    "            neg_energy_fn(list(params.values()), x_batch, y_batch)\n",
    "            * (target.shape[0] / y_batch.shape[0])\n",
    "            / kl_weight\n",
    "        )\n",
    "    def _target_log_prob_fn(params, features, target, seed, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            return _target_log_prob_fn_inner(params, features, target)\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        indices = torch.randperm(features.size(0))[:batch_size]\n",
    "\n",
    "        batch_features = features[indices]\n",
    "        batch_target = target[indices]\n",
    "\n",
    "        return _target_log_prob_fn_inner(params, batch_features, batch_target)\n",
    "    def make_surrogate_posterior(params: dict, batch_ndims=1):\n",
    "        posterior = {}\n",
    "\n",
    "        keys = list(params.keys())\n",
    "\n",
    "        for i in range(0, len(keys), 2):\n",
    "            mean_key = keys[i]\n",
    "            log_std_key = keys[i + 1]\n",
    "\n",
    "            mean = params[mean_key]\n",
    "            log_std = params[log_std_key]\n",
    "            std = 0.0001 + F.softplus(log_std)\n",
    "            posterior[mean_key] = Normal(mean, std)\n",
    "\n",
    "        return posterior\n",
    "\n",
    "    def sample_from_posterior(posterior, num_samples=10):\n",
    "\n",
    "        samples = {}\n",
    "        for key, dist in posterior.items():\n",
    "            samples[key] = dist.sample((num_samples,))  \n",
    "        return samples\n",
    "    init_seeds = torch.randint(0, 2**32 - 1, (1, ensemble_size), dtype=torch.int64)\n",
    "    init_params = [init_fn(seed.item()) for seed in init_seeds[0]]\n",
    "    def fit_surrogate_posterior(\n",
    "        init_params, target_log_prob_fn, learning_rate, num_epochs, sample_size, batch_size\n",
    "    ):\n",
    "        optimizer = torch.optim.Adam([p for p in init_params.values() if isinstance(p, torch.Tensor)], lr=learning_rate)\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            posterior = make_surrogate_posterior(init_params)\n",
    "            samples = sample_from_posterior(posterior, 1)\n",
    "            loss = -target_log_prob_fn(samples, features, target, seed, batch_size)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return init_params, torch.tensor(losses)\n",
    "\n",
    "    def predict(surrogate_params, num_samples):\n",
    "        posterior = make_surrogate_posterior(surrogate_params)\n",
    "        predictions = sample_from_posterior(posterior, num_samples)\n",
    "        return predictions\n",
    "\n",
    "    opt_seeds = torch.randint(0, 2**32 - 1, (ensemble_size,), dtype=torch.int64)\n",
    "\n",
    "    fit_params = []\n",
    "    fit_losses = []\n",
    "\n",
    "    for seed in opt_seeds:\n",
    "        trained_params, train_losses = fit_surrogate_posterior(\n",
    "            init_params[seed % ensemble_size], \n",
    "            _target_log_prob_fn, \n",
    "            learning_rate, \n",
    "            num_epochs, \n",
    "            sample_size, \n",
    "            batch_size\n",
    "        )\n",
    "        fit_params.append(trained_params)\n",
    "        fit_losses.append(train_losses)\n",
    "\n",
    "    predictions = [predict(params, num_samples) for params in fit_params]\n",
    "\n",
    "    losses = torch.stack(fit_losses)\n",
    "    predictions = {key: torch.stack([p[key] for p in predictions]) for key in predictions[0]}\n",
    "\n",
    "    return fit_params, losses, predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Updated)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
